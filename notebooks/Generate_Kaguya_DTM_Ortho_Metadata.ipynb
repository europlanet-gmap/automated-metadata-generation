{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prompt-calibration",
   "metadata": {},
   "source": [
    "# Generate Kaguya DTM & Ortho Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-newsletter",
   "metadata": {},
   "source": [
    "First we have the necessart imports. These are largely encapsulated inside of the automated metadata generation (`amg`) library. THe only external things we import are `json` (so that we can write out STAC files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "conventional-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "from amg.isismetadata import IsisMetadata\n",
    "from amg.fgdcmetadata import FGDCMetadata, OrthographicFgdcParser\n",
    "from amg.gdalmetadata import GDALMetadata\n",
    "from amg.databasemetadata import DbMetadata\n",
    "from amg.plaintextmetadata import PcAlignMetadata\n",
    "from amg.yamlmetadata import YAMLMetadata\n",
    "from amg.formatters.stac_formatter import to_stac\n",
    "from amg.formatters.fgdc_formatter import to_fgdc\n",
    "from amg.utils import find_file, write_fgdc, write_stac\n",
    "from amg import UnifiedMetadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-milwaukee",
   "metadata": {},
   "source": [
    "## Step I: Stage the data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wanted-newsletter",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "import fileinput\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "DESTINATION_DIR = '/scratch/ARD/stac/moon/'\n",
    "BASEDIR = '/scratch/ARD/processed/kaguya_dtms/KAGTC_v2/'\n",
    "\n",
    "def copy_file(source, destination):\n",
    "    if not os.path.exists(os.path.join(destination, os.path.basename(source))):\n",
    "        shutil.copy2(source, destination)\n",
    "    assert os.path.exists(destination)\n",
    "\n",
    "def stage_stereo_ortho(datadir, dtm_subpath='ctx_dtms'):\n",
    "    # Find the data\n",
    "    dem = find_file(datadir, 'results_ba', '*_ba-aligned-DEM-cog.tif')\n",
    "    dem_thumbnail = find_file(datadir, 'results_ba', '*_ba-aligned-DEM-cog.jpg')\n",
    "    intersection_err = find_file(datadir, 'results_ba', '*_ba-aligned-IntersectionErr.tif')\n",
    "    ortho = find_file(datadir, 'results_ba', '*_ba-aligned-DRG-cog.tif')\n",
    "    ortho_thumbnail = find_file(datadir, 'results_ba', '*_ba-aligned-DRG-cog.jpg')\n",
    "    hillshade = find_file(datadir, 'results_ba', '*_ba-aligned-hs.tif')\n",
    "    qa_metrics = find_file(datadir, 'results_ba', 'qa_metrics.txt')\n",
    "    asp_provenance = find_file(datadir, 'results_ba', 'asp_provenance.txt')\n",
    "    isis_provenance = find_file(datadir, 'isis_provenance.txt')\n",
    "\n",
    "    if None in [dem, ortho]:\n",
    "        \n",
    "        # Processing failed\n",
    "        return\n",
    "    \n",
    "    # Generate the output directory for the data to be staged\n",
    "    pairname = os.path.basename(datadir)\n",
    "    dtm_destination_dir = os.path.join(DESTINATION_DIR, dtm_subpath, pairname)\n",
    "    if not os.path.exists(dtm_destination_dir):\n",
    "        os.mkdir(dtm_destination_dir)\n",
    "       \n",
    "    # Move the DTM data\n",
    "    data_to_move = []\n",
    "    for data in [dem, dem_thumbnail, \n",
    "                 qa_metrics, \n",
    "                 ortho, ortho_thumbnail, \n",
    "                 hillshade]:\n",
    "        if 'aligned-hs' in data:\n",
    "            outname = os.path.basename(data).replace('_ba-aligned-hs', '_HILLSHADE')\n",
    "        elif 'DEM' in data:\n",
    "            outname = os.path.basename(data).replace('_ba-aligned-DEM-cog', '_DEM')\n",
    "        elif 'DRG' in data:\n",
    "            outname = os.path.basename(data).replace('_ba-aligned-DRG-cog', '_ORTHO')\n",
    "        elif 'Intersection' in data:\n",
    "            outname = os.path.basename(data).replace('_ba-aligned-', '_')\n",
    "        else:\n",
    "            outname = os.path.basename(data)\n",
    "        data_renamed = os.path.join(dtm_destination_dir, outname)\n",
    "        data_to_move.append((data, data_renamed))\n",
    "    \n",
    "    for source, destination in data_to_move:\n",
    "        copy_file(source, destination)\n",
    "    \n",
    "    destination = os.path.join(dtm_destination_dir, outname)\n",
    "    with fileinput.input(files=(isis_provenance, asp_provenance)) as f:\n",
    "        with open(os.path.join(dtm_destination_dir, 'provenance.txt'), 'w') as p:\n",
    "            for line in f:\n",
    "                p.write(line)\n",
    "\n",
    "for datadir in glob.glob(BASEDIR + '*'):\n",
    "    #try:\n",
    "    stage_stereo_ortho(datadir, dtm_subpath='kaguyatc_dtms')\n",
    "    #except:\n",
    "    #    print(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-rotation",
   "metadata": {},
   "source": [
    "⭐️⭐️⭐️ If the STAC collection / catalog do not already exist, they need to be created **before** pushing stac messages to SQS. The [ARD repository](https://github.com/USGS-Astrogeology/ARD_STAC) has the full hierarchal organization of the STAC catalogs and collections. ⭐️⭐️⭐️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-sydney",
   "metadata": {},
   "source": [
    "### Step II: Generate the STAC and FGDC metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wanted-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGED_DTMS = '/scratch/ARD/stac/moon/kaguyatc_dtms/'\n",
    "OUTFILE = '2021_6_29.lis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "secret-decline",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/ARD/stac/moon/kaguyatc_dtms/2021_6_29.lis\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "stac_files = []\n",
    "def create_unified_metadata_obj(basename, dtm, ortho, qa_metrics):\n",
    "    fgdc = FGDCMetadata('../templates/kaguyatc_dtm_template.xml', proj='orthogr')\n",
    "    gd = GDALMetadata(dtm)\n",
    "    image_a = basename.split('__')[0]\n",
    "    image_b = basename.split('__')[1]\n",
    "    sql = f\"\"\"\n",
    "    WITH cte_geoms AS\n",
    "        (\n",
    "        SELECT id, geom FROM kaguyatc\n",
    "        WHERE \n",
    "            kaguyatc.name LIKE ANY (array ['{image_a}%%', '{image_b}%%'])\n",
    "        )\n",
    "    SELECT ST_AsText(ST_Extent(ST_Intersection(A.geom, B.geom))) FROM cte_geoms as A, cte_geoms as B\n",
    "    WHERE A.id > B.id AND A.id != B.id\n",
    "    \"\"\"\n",
    "    db = DbMetadata('moon', 'postgresql://jay:abcde@autocnet.wr.usgs.gov:30001', sql=sql)\n",
    "    vrt = PcAlignMetadata(qa)\n",
    "    yml = YAMLMetadata('../templates/kaguyadtm.yml')\n",
    "    \n",
    "    # Define overrides\n",
    "    dtm_overrides = {\n",
    "                 'basename':basename,\n",
    "                 'href':f'https://asc-moon.s3-us-west-2.amazonaws.com/kaguyatc_dtms/{basename}',\n",
    "                 'title':f'Kaguya Terrain Camera Digital Terrain Model; Moon; {image_a}, {image_b}'}\n",
    "    \n",
    "    # Define mappings\n",
    "    mappings = {'bbox':DbMetadata, 'footprint':DbMetadata, 'title':YAMLMetadata,\n",
    "                'license':YAMLMetadata, }\n",
    "\n",
    "    # Create a unified DTM metadata object\n",
    "    dtm_record = UnifiedMetadata([fgdc, gd, db, vrt, yml], mappings=mappings, overrides=dtm_overrides)\n",
    "    \n",
    "    return dtm_record\n",
    "\n",
    "for i, basedir in enumerate(glob.glob(STAGED_DTMS + '/*')):\n",
    "    # This is all file manipulation to make sure data are staged properly\n",
    "    if 'collection.json' in basedir:\n",
    "        continue\n",
    "    dtm = find_file(basedir,  '*DEM.tif')\n",
    "    ortho = find_file(basedir, '*ORTHO.tif')\n",
    "    qa = find_file(basedir, 'qa_metrics.txt')\n",
    "    if qa is None:\n",
    "        print(basedir)\n",
    "        continue\n",
    "        \n",
    "    # This builds the metadata record\n",
    "    dtm_record = create_unified_metadata_obj(os.path.basename(basedir), dtm, ortho, qa)\n",
    "        \n",
    "    # Generate and write FGDC\n",
    "    dtm_name = os.path.splitext(dtm)[0] \n",
    "    write_fgdc(os.path.join(basedir, dtm_name + '.xml'), to_fgdc(dtm_record))\n",
    "    \n",
    "    # Manually define the assets for this set of products\n",
    "    dtm_assets = [{'title':'DEM Thumbnail',\n",
    "           'href':'{href}/{productid}.jpg',\n",
    "           'type':'image/jpeg',\n",
    "           'roles':['thumbnail'],\n",
    "           'key':'thumbnail'},\n",
    "          {'title': 'DEM',\n",
    "           'href':'{href}/{productid}.tif',\n",
    "           'type':'image/tiff; application=geotiff; profile=cloud-optimized',\n",
    "           'roles':['data'],\n",
    "           'key':'dem'},\n",
    "          {'title':'Hillshade',\n",
    "           'href':'{href}/{basename}_HILLSHADE.tif',\n",
    "           'type':'image/tiff; application=geotiff; profile=cloud-optimized',\n",
    "           'roles':['data'],\n",
    "           'key':'hillshade'},\n",
    "          {'title':'Orthoimage',\n",
    "           'href':'{href}/{basename}_ORTHO.tif',\n",
    "           'type':'image/tiff; application=geotiff; profile=cloud-optimized',\n",
    "           'roles':['data'],\n",
    "           'key':'ortho image'},\n",
    "          {'title': 'DTM FGDC Metadata',\n",
    "           'href':'{href}/{productid}.xml',\n",
    "           'type':'application/xml',\n",
    "           'roles':['metadata'],\n",
    "           'key':'fgdc_metadata_dtm'},\n",
    "          {'title': 'Quality Assurance Metrics',\n",
    "           'href': '{href}/qa_metrics.txt',\n",
    "           'type':'text/plain',\n",
    "           'roles': ['metadata'],\n",
    "           'key':'qa_metric'},\n",
    "          {'title': 'ASP generated intersection error raster',\n",
    "           'href':'{href}/{basename}_IntersectionErr.tif',\n",
    "           'type':'image/tiff; application=geotiff',\n",
    "           'roles':['metadata', 'data-mask'],\n",
    "           'key':'intersection_err'},\n",
    "          {'title':'Processing steps in ISIS and ASP used to generate the data product',\n",
    "           'href':'{href}/provenance.txt',\n",
    "           'type':'text/plain',\n",
    "           'roles':['metadata'],\n",
    "           'key':'provenance'\n",
    "          }]\n",
    "    \n",
    "    # Generate and write STAC\n",
    "    stac_dtm = to_stac(dtm_record, assets=dtm_assets)\n",
    "    stac_file = os.path.join(basedir, dtm_name + '.json')\n",
    "    write_stac(stac_file, stac_dtm)\n",
    "    \n",
    "    # Append the stac file path to the list of file paths\n",
    "    stac_files.append(stac_file)\n",
    "\n",
    "# Write the list of created STAC files to the outfile\n",
    "with open(os.path.join(STAGED_DTMS, OUTFILE), 'w') as f:\n",
    "    for stac_file in stac_files:\n",
    "        f.write(stac_file + '\\n')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-minnesota",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amg",
   "language": "python",
   "name": "amg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
